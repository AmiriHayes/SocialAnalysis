{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fe29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/Cornell-University/arxiv?dataset_version_number=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.51G/1.51G [02:54<00:00, 9.33MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: C:\\Users\\amkah\\.cache\\kagglehub\\datasets\\Cornell-University\\arxiv\\versions\\256\n",
      "JSON snapshot path: C:\\Users\\amkah\\.cache\\kagglehub\\datasets\\Cornell-University\\arxiv\\versions\\256\\arxiv-metadata-oai-snapshot.json\n",
      "Subset shape: (10000, 14)\n",
      "         id                                              title      categories\n",
      "0  704.0001  Calculation of prompt diphoton production cros...          hep-ph\n",
      "1  704.0002           Sparsity-certifying Graph Decompositions   math.CO cs.CG\n",
      "2  704.0003  The evolution of the Earth-Moon system based o...  physics.gen-ph\n",
      "Saved trimmed subset → data/arxiv_subset_10k.json\n",
      "Deleted large original file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "file_path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "print(\"Downloaded to:\", file_path)\n",
    "json_path = os.path.join(file_path, \"arxiv-metadata-oai-snapshot.json\")\n",
    "print(\"JSON snapshot path:\", json_path)\n",
    "\n",
    "subset_df = pd.read_json(json_path, lines=True, nrows=10_000)\n",
    "print(\"Subset shape:\", subset_df.shape)\n",
    "print(subset_df[[\"id\", \"title\", \"categories\"]].head(3))\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "subset_path = \"data/arxiv_subset_10k.jsonl\"\n",
    "subset_df.to_json(subset_path, orient=\"records\", lines=True)\n",
    "print(f\"Saved trimmed subset → {subset_path}\")\n",
    "\n",
    "os.remove(json_path)\n",
    "print(\"Deleted large original file.\")\n",
    "df = pd.read_json(\"data/arxiv_subset_10k.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cc1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT model loaded for embedding abstracts.\n",
      "\n",
      "\n",
      "categories\n",
      "hep-th                            7\n",
      "hep-ph                            6\n",
      "astro-ph                          6\n",
      "physics.optics physics.comp-ph    4\n",
      "cond-mat.mes-hall                 3\n",
      "gr-qc                             3\n",
      "math.CO                           3\n",
      "math.NT                           2\n",
      "cond-mat.mtrl-sci                 2\n",
      "math.CA math.FA                   2\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"SBERT model loaded for embedding abstracts.\\n\\n\")\n",
    "print(df[\"categories\"].value_counts().head(10), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f75e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(G, fname):\n",
    "    data = {\n",
    "        \"nodes\": list(G.nodes(data=True)), \n",
    "        \"edges\": list(G.edges(data=True))\n",
    "    }\n",
    "    with open(fname, 'w') as f: json.dump(data, f, indent=2)\n",
    "\n",
    "def load(fname):\n",
    "    G = nx.DiGraph()\n",
    "    d = json.load(open(fname))\n",
    "    G.add_nodes_from(d['nodes'])\n",
    "    G.add_edges_from(d['edges'])\n",
    "    return G\n",
    "\n",
    "def handle_author(author_parsed_instance):\n",
    "    # convert [[\"Ortega-Cerda\",\"Joaquim\",\"\"]] to Joa.Ortega-Cerda\n",
    "    first = author_parsed_instance[1]\n",
    "    last = author_parsed_instance[0]\n",
    "    author_clean = f\"{first}|{last}\"\n",
    "    return author_clean\n",
    "\n",
    "def generate_tag(df, node_type=\"article\", edge_type=\"cites\", out_dir=\"data\", limit=None):\n",
    "    if limit: df = df.head(limit)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    G = nx.Graph(name=f\"TAG_{node_type}_{edge_type}\")\n",
    "\n",
    "    # NODE CONSTRUCTION\n",
    "    if node_type == \"article\":\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding article nodes\"):\n",
    "            node_id = row[\"id\"]\n",
    "            text = row[\"title\"] + \":\\n\" + row[\"abstract\"]\n",
    "            G.add_node(\n",
    "                node_id,\n",
    "                type=\"article\",\n",
    "                text=text,\n",
    "                embedding=embedder.encode(text).tolist(),\n",
    "                category=row[\"categories\"],\n",
    "            )\n",
    "    elif node_type == \"author\":\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding author nodes\"):\n",
    "            authors = row[\"authors_parsed\"]\n",
    "            for author in authors:\n",
    "                author_clean = handle_author(author)\n",
    "                if not G.has_node(author_clean):\n",
    "                    G.add_node(author_clean, type=\"author\", embedding=None)\n",
    "\n",
    "    # EDGE CONSTRUCTION\n",
    "\n",
    "    if edge_type == \"coauthor\":\n",
    "        if node_type != \"author\": \n",
    "            print(\"You made a mistake. Coauthor edges require author nodes. Change node_type to 'author'.\")\n",
    "            return None\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding coauthor edges\"):\n",
    "            authors = row[\"authors_parsed\"]\n",
    "            for i, a1 in enumerate(authors):\n",
    "                author1_clean = handle_author(a1)\n",
    "                for a2 in authors[i + 1:]:\n",
    "                    author2_clean = handle_author(a2)\n",
    "                    G.add_edge(author1_clean, author2_clean, type=\"coauthor\", paper=row[\"id\"])\n",
    "                    \n",
    "    elif edge_type == \"cites\":\n",
    "        ids = df[\"id\"].tolist()\n",
    "        for i in range(len(ids) - 1):\n",
    "            G.add_edge(ids[i], ids[i + 1], type=\"cites\")\n",
    "\n",
    "    elif edge_type == \"co_citation\":\n",
    "        for cat, group in df.groupby(\"categories\"):\n",
    "            ids = group[\"id\"].tolist()\n",
    "            for i in range(len(ids) - 1):\n",
    "                G.add_edge(ids[i], ids[i + 1], type=\"co_citation\")\n",
    "\n",
    "    print(G)\n",
    "\n",
    "    # SAVE STUFF\n",
    "    out_path = os.path.join(out_dir, f\"TAG_{node_type}_{edge_type}.json\")\n",
    "    save(G, out_path)\n",
    "    print(f\"✅ Saved {len(G)} nodes, {G.number_of_edges()} edges → {out_path}\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD [1]: COAUTHORSHIP GRAPH\n",
    "\n",
    "G_coauthorship = nx.Graph()\n",
    "article_data = {}\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building graph topology and external repo\"):\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    abstract = row[\"abstract\"]\n",
    "    article_data[article_id] = {\n",
    "        \"abstract\": title + \"\\n\" + abstract,\n",
    "        \"vector\": None\n",
    "    }\n",
    "    authors = row[\"authors_parsed\"]\n",
    "    for i, a1 in enumerate(authors):\n",
    "        author1_clean = handle_author(a1)\n",
    "        for a2 in authors[i + 1:]:\n",
    "            author2_clean = handle_author(a2)\n",
    "            # ADD NODES\n",
    "            if not G_coauthorship.has_node(author1_clean): G_coauthorship.add_node(author1_clean, type=\"author\")\n",
    "            if not G_coauthorship.has_node(author2_clean): G_coauthorship.add_node(author2_clean, type=\"author\")\n",
    "\n",
    "            # ADD/UPDATE EDGES\n",
    "            if G_coauthorship.has_edge(author1_clean, author2_clean):\n",
    "                G_coauthorship[author1_clean][author2_clean][\"paper_ids\"].append(article_id)\n",
    "                G_coauthorship[author1_clean][author2_clean][\"weight\"] += 1\n",
    "            else:\n",
    "                G_coauthorship.add_edge(author1_clean, author2_clean, paper_ids=[article_id], weight=1)\n",
    "\n",
    "for article_id, data in tqdm(article_data.items(), desc=\"Generating SBERT embeddings\"):\n",
    "    abstract = data[\"abstract\"]\n",
    "    vector = embedder.encode(abstract)\n",
    "    article_data[article_id][\"vector\"] = vector\n",
    "\n",
    "for u, v, data in tqdm(G_coauthorship.edges(data=True), desc=\"Enriching graph edges with topic embeddings\"):\n",
    "    paper_ids = data[\"paper_ids\"]\n",
    "    vectors_to_average = []\n",
    "    for pid in paper_ids:\n",
    "        vector = article_data[pid][\"vector\"]\n",
    "        vectors_to_average.append(vector)\n",
    "    mean_vector = sum(vectors_to_average) / len(vectors_to_average)\n",
    "    data[\"topic_embedding\"] = mean_vector.tolist()\n",
    "\n",
    "embeddings = []\n",
    "for article_id, data in article_data.items():\n",
    "    vector = data[\"vector\"]\n",
    "    if vector is not None:\n",
    "        embeddings.append(vector)\n",
    "\n",
    "save(G_coauthorship, \"data/TAG_author_coauthor_enriched.json\")\n",
    "embeddings_array = np.array(embeddings)\n",
    "np.save(\"data/article_embeddings.npy\", embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD [2]: COCITATION GRAPH\n",
    "\n",
    "G_cocitation = nx.Graph()\n",
    "article_data = {}\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building graph topology and external repo\"):\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    abstract = row[\"abstract\"]\n",
    "    article_data[article_id] = {\n",
    "        \"abstract\": title + \"\\n\" + abstract,\n",
    "        \"vector\": None\n",
    "    }\n",
    "    categories = row[\"categories\"].split()\n",
    "    for i, cat1 in enumerate(categories):\n",
    "        for cat2 in categories[i + 1:]:\n",
    "            # ADD NODES\n",
    "            if not G_cocitation.has_node(cat1): G_cocitation.add_node(cat1, type=\"category\")\n",
    "            if not G_cocitation.has_node(cat2): G_cocitation.add_node(cat2, type=\"category\")\n",
    "\n",
    "            # ADD/UPDATE EDGES\n",
    "            if G_cocitation.has_edge(cat1, cat2):\n",
    "                G_cocitation[cat1][cat2][\"paper_ids\"].append(article_id)\n",
    "                G_cocitation[cat1][cat2][\"weight\"] += 1\n",
    "            else:\n",
    "                G_cocitation.add_edge(cat1, cat2, paper_ids=[article_id], weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "248b858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n",
      "       'report-no', 'categories', 'license', 'abstract', 'versions',\n",
      "       'update_date', 'authors_parsed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a0ba03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save article embeddings from article_data as numpy file\n",
    "import numpy as np\n",
    "\n",
    "# Extract article embeddings and save as numpy array\n",
    "embeddings = []\n",
    "for article_id, data in article_data.items():\n",
    "    vector = data[\"vector\"]\n",
    "    if vector is not None:\n",
    "        embeddings.append(vector)\n",
    "\n",
    "# Convert to numpy array and save\n",
    "embeddings_array = np.array(embeddings)\n",
    "np.save(\"data/article_embeddings.npy\", embeddings_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6b5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
